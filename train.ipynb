{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "game_name = 'BreakoutNoFrameskip-v4' # 'Breakout-v0' # 'VideoPinball-v0'\n",
    "device = 'cuda:0'\n",
    "chkpnt_name = 'breakoutv4_1000_2'\n",
    "\n",
    "agent = Agent(game_name, device, chkpnt_path)\n",
    "\n",
    "plt.imshow(agent.sanity_check_screen())\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----------------------------------------\n",
      "Episode  0\n",
      "Steps taken:  126\n",
      "Cumulative Steps taken:  126\n",
      "Loss:  0.006325353257923093\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  1\n",
      "Steps taken:  125\n",
      "Cumulative Steps taken:  251\n",
      "Loss:  0.011152276177595455\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  2\n",
      "Steps taken:  125\n",
      "Cumulative Steps taken:  376\n",
      "Loss:  0.02545809265923265\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  3\n",
      "Steps taken:  126\n",
      "Cumulative Steps taken:  502\n",
      "Loss:  0.0034663347571941773\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  4\n",
      "Steps taken:  124\n",
      "Cumulative Steps taken:  626\n",
      "Loss:  0.042713523905468814\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  5\n",
      "Steps taken:  157\n",
      "Cumulative Steps taken:  783\n",
      "Loss:  0.0017857021023755962\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  6\n",
      "Steps taken:  123\n",
      "Cumulative Steps taken:  906\n",
      "Loss:  0.0004078491838310294\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  7\n",
      "Steps taken:  122\n",
      "Cumulative Steps taken:  1028\n",
      "Loss:  0.024565784402022645\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  8\n",
      "Steps taken:  139\n",
      "Cumulative Steps taken:  1167\n",
      "Loss:  0.0051390844767815365\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  9\n",
      "Steps taken:  214\n",
      "Cumulative Steps taken:  1381\n",
      "Loss:  0.036659191095393374\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  10\n",
      "Steps taken:  245\n",
      "Cumulative Steps taken:  1626\n",
      "Loss:  0.003944186068016325\n",
      "Reward:  3.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  11\n",
      "Steps taken:  194\n",
      "Cumulative Steps taken:  1820\n",
      "Loss:  0.000271733069295605\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  12\n",
      "Steps taken:  127\n",
      "Cumulative Steps taken:  1947\n",
      "Loss:  0.03815668320991528\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  13\n",
      "Steps taken:  160\n",
      "Cumulative Steps taken:  2107\n",
      "Loss:  0.00037878139262942107\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  14\n",
      "Steps taken:  219\n",
      "Cumulative Steps taken:  2326\n",
      "Loss:  0.08579794200785146\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  15\n",
      "Steps taken:  281\n",
      "Cumulative Steps taken:  2607\n",
      "Loss:  0.00022866474510415912\n",
      "Reward:  4.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  16\n",
      "Steps taken:  137\n",
      "Cumulative Steps taken:  2744\n",
      "Loss:  0.06960885163788263\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  17\n",
      "Steps taken:  173\n",
      "Cumulative Steps taken:  2917\n",
      "Loss:  0.001728462409180169\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  18\n",
      "Steps taken:  197\n",
      "Cumulative Steps taken:  3114\n",
      "Loss:  0.0009461623177027764\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  19\n",
      "Steps taken:  203\n",
      "Cumulative Steps taken:  3317\n",
      "Loss:  0.006975623592279644\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  20\n",
      "Steps taken:  165\n",
      "Cumulative Steps taken:  3482\n",
      "Loss:  0.0006166097037494759\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  21\n",
      "Steps taken:  129\n",
      "Cumulative Steps taken:  3611\n",
      "Loss:  0.10780173300954625\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  22\n",
      "Steps taken:  189\n",
      "Cumulative Steps taken:  3800\n",
      "Loss:  0.0003622414012183821\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  23\n",
      "Steps taken:  161\n",
      "Cumulative Steps taken:  3961\n",
      "Loss:  0.013565832219717744\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  24\n",
      "Steps taken:  214\n",
      "Cumulative Steps taken:  4175\n",
      "Loss:  0.0017968043227116158\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  25\n",
      "Steps taken:  135\n",
      "Cumulative Steps taken:  4310\n",
      "Loss:  0.00011947306556512235\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  26\n",
      "Steps taken:  170\n",
      "Cumulative Steps taken:  4480\n",
      "Loss:  0.0012756913581573374\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  27\n",
      "Steps taken:  157\n",
      "Cumulative Steps taken:  4637\n",
      "Loss:  0.05105495611974574\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  28\n",
      "Steps taken:  166\n",
      "Cumulative Steps taken:  4803\n",
      "Loss:  0.00025724056477733595\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  29\n",
      "Steps taken:  172\n",
      "Cumulative Steps taken:  4975\n",
      "Loss:  0.06286113618685726\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  30\n",
      "Steps taken:  125\n",
      "Cumulative Steps taken:  5100\n",
      "Loss:  0.0012075113832958318\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  31\n",
      "Steps taken:  366\n",
      "Cumulative Steps taken:  5466\n",
      "Loss:  0.004283028530454454\n",
      "Reward:  6.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  32\n",
      "Steps taken:  134\n",
      "Cumulative Steps taken:  5600\n",
      "Loss:  0.0012475895342006131\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  33\n",
      "Steps taken:  222\n",
      "Cumulative Steps taken:  5822\n",
      "Loss:  0.008698257096342522\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  34\n",
      "Steps taken:  172\n",
      "Cumulative Steps taken:  5994\n",
      "Loss:  0.07590938249077672\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  35\n",
      "Steps taken:  137\n",
      "Cumulative Steps taken:  6131\n",
      "Loss:  0.0451638833065224\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  36\n",
      "Steps taken:  133\n",
      "Cumulative Steps taken:  6264\n",
      "Loss:  0.00023889840165013126\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  37\n",
      "Steps taken:  155\n",
      "Cumulative Steps taken:  6419\n",
      "Loss:  0.013948131884445138\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  38\n",
      "Steps taken:  204\n",
      "Cumulative Steps taken:  6623\n",
      "Loss:  0.05044668719410705\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  39\n",
      "Steps taken:  129\n",
      "Cumulative Steps taken:  6752\n",
      "Loss:  0.05208693500058601\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  40\n",
      "Steps taken:  161\n",
      "Cumulative Steps taken:  6913\n",
      "Loss:  0.005868790748557513\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  41\n",
      "Steps taken:  156\n",
      "Cumulative Steps taken:  7069\n",
      "Loss:  0.0004568900866991056\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  42\n",
      "Steps taken:  154\n",
      "Cumulative Steps taken:  7223\n",
      "Loss:  0.0020894120012803\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  43\n",
      "Steps taken:  175\n",
      "Cumulative Steps taken:  7398\n",
      "Loss:  0.00010551591799028132\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  44\n",
      "Steps taken:  133\n",
      "Cumulative Steps taken:  7531\n",
      "Loss:  0.048160792258661475\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  45\n",
      "Steps taken:  255\n",
      "Cumulative Steps taken:  7786\n",
      "Loss:  0.0006765422334665814\n",
      "Reward:  3.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  46\n",
      "Steps taken:  186\n",
      "Cumulative Steps taken:  7972\n",
      "Loss:  0.011165853332039921\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  47\n",
      "Steps taken:  127\n",
      "Cumulative Steps taken:  8099\n",
      "Loss:  0.009105329975152825\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  48\n",
      "Steps taken:  208\n",
      "Cumulative Steps taken:  8307\n",
      "Loss:  0.0006870701594716099\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  49\n",
      "Steps taken:  193\n",
      "Cumulative Steps taken:  8500\n",
      "Loss:  0.00015444620462950738\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  50\n",
      "Steps taken:  132\n",
      "Cumulative Steps taken:  8632\n",
      "Loss:  0.045840671117238455\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  51\n",
      "Steps taken:  183\n",
      "Cumulative Steps taken:  8815\n",
      "Loss:  0.00045961041527532037\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  52\n",
      "Steps taken:  176\n",
      "Cumulative Steps taken:  8991\n",
      "Loss:  0.03222489411419165\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  53\n",
      "Steps taken:  230\n",
      "Cumulative Steps taken:  9221\n",
      "Loss:  0.012853287421470006\n",
      "Reward:  3.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  54\n",
      "Steps taken:  128\n",
      "Cumulative Steps taken:  9349\n",
      "Loss:  0.00024163049363592657\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  55\n",
      "Steps taken:  129\n",
      "Cumulative Steps taken:  9478\n",
      "Loss:  0.09969694679351951\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  56\n",
      "Steps taken:  133\n",
      "Cumulative Steps taken:  9611\n",
      "Loss:  0.0320871345661437\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps taken:  164\n",
      "Cumulative Steps taken:  9775\n",
      "Loss:  0.030273970785816576\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  58\n",
      "Steps taken:  132\n",
      "Cumulative Steps taken:  9907\n",
      "Loss:  0.007501322192134784\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  59\n",
      "Steps taken:  197\n",
      "Cumulative Steps taken:  10104\n",
      "Loss:  0.00044240645145837004\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  60\n",
      "Steps taken:  181\n",
      "Cumulative Steps taken:  10285\n",
      "Loss:  0.0004263135106396675\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  61\n",
      "Steps taken:  141\n",
      "Cumulative Steps taken:  10426\n",
      "Loss:  0.00023484155892050223\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  62\n",
      "Steps taken:  143\n",
      "Cumulative Steps taken:  10569\n",
      "Loss:  8.369882491558115e-06\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  63\n",
      "Steps taken:  166\n",
      "Cumulative Steps taken:  10735\n",
      "Loss:  0.00013985714357104536\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  64\n",
      "Steps taken:  216\n",
      "Cumulative Steps taken:  10951\n",
      "Loss:  0.0002344839673498381\n",
      "Reward:  3.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  65\n",
      "Steps taken:  195\n",
      "Cumulative Steps taken:  11146\n",
      "Loss:  0.00010569203126797096\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  66\n",
      "Steps taken:  129\n",
      "Cumulative Steps taken:  11275\n",
      "Loss:  9.658461213672793e-05\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  67\n",
      "Steps taken:  228\n",
      "Cumulative Steps taken:  11503\n",
      "Loss:  0.03190084983509574\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  68\n",
      "Steps taken:  202\n",
      "Cumulative Steps taken:  11705\n",
      "Loss:  0.031563112703290486\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  69\n",
      "Steps taken:  218\n",
      "Cumulative Steps taken:  11923\n",
      "Loss:  1.7213341709580424e-05\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  70\n",
      "Steps taken:  205\n",
      "Cumulative Steps taken:  12128\n",
      "Loss:  7.467777575329159e-06\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  71\n",
      "Steps taken:  123\n",
      "Cumulative Steps taken:  12251\n",
      "Loss:  0.00028578177768087297\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  72\n",
      "Steps taken:  190\n",
      "Cumulative Steps taken:  12441\n",
      "Loss:  0.00012641087203659634\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  73\n",
      "Steps taken:  278\n",
      "Cumulative Steps taken:  12719\n",
      "Loss:  5.1419657950599084e-05\n",
      "Reward:  4.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  74\n",
      "Steps taken:  127\n",
      "Cumulative Steps taken:  12846\n",
      "Loss:  0.031237100971973183\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  75\n",
      "Steps taken:  132\n",
      "Cumulative Steps taken:  12978\n",
      "Loss:  0.0001918815497816222\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  76\n",
      "Steps taken:  204\n",
      "Cumulative Steps taken:  13182\n",
      "Loss:  0.031361568531131596\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  77\n",
      "Steps taken:  201\n",
      "Cumulative Steps taken:  13383\n",
      "Loss:  0.0006124714981246789\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  78\n",
      "Steps taken:  186\n",
      "Cumulative Steps taken:  13569\n",
      "Loss:  0.00025290230032740384\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  79\n",
      "Steps taken:  158\n",
      "Cumulative Steps taken:  13727\n",
      "Loss:  9.043604766224912e-05\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  80\n",
      "Steps taken:  239\n",
      "Cumulative Steps taken:  13966\n",
      "Loss:  1.9763299590785604e-05\n",
      "Reward:  3.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  81\n",
      "Steps taken:  177\n",
      "Cumulative Steps taken:  14143\n",
      "Loss:  0.031220786132173075\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  82\n",
      "Steps taken:  204\n",
      "Cumulative Steps taken:  14347\n",
      "Loss:  0.03038805875533588\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  83\n",
      "Steps taken:  159\n",
      "Cumulative Steps taken:  14506\n",
      "Loss:  7.035105270405461e-05\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  84\n",
      "Steps taken:  136\n",
      "Cumulative Steps taken:  14642\n",
      "Loss:  0.03101760107058592\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  85\n",
      "Steps taken:  154\n",
      "Cumulative Steps taken:  14796\n",
      "Loss:  9.126512502433278e-05\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  86\n",
      "Steps taken:  157\n",
      "Cumulative Steps taken:  14953\n",
      "Loss:  0.0003533365739339016\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  87\n",
      "Steps taken:  124\n",
      "Cumulative Steps taken:  15077\n",
      "Loss:  7.370861639360702e-05\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  88\n",
      "Steps taken:  207\n",
      "Cumulative Steps taken:  15284\n",
      "Loss:  0.0002900814049675044\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  89\n",
      "Steps taken:  251\n",
      "Cumulative Steps taken:  15535\n",
      "Loss:  0.00022955211863561067\n",
      "Reward:  3.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  90\n",
      "Steps taken:  125\n",
      "Cumulative Steps taken:  15660\n",
      "Loss:  3.845387683282394e-05\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  91\n",
      "Steps taken:  139\n",
      "Cumulative Steps taken:  15799\n",
      "Loss:  6.71652312159238e-05\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  92\n",
      "Steps taken:  180\n",
      "Cumulative Steps taken:  15979\n",
      "Loss:  0.03275218804329644\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  93\n",
      "Steps taken:  133\n",
      "Cumulative Steps taken:  16112\n",
      "Loss:  0.0003711483330421678\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  94\n",
      "Steps taken:  175\n",
      "Cumulative Steps taken:  16287\n",
      "Loss:  0.03111117345475926\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  95\n",
      "Steps taken:  129\n",
      "Cumulative Steps taken:  16416\n",
      "Loss:  0.0001227001600487515\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  96\n",
      "Steps taken:  151\n",
      "Cumulative Steps taken:  16567\n",
      "Loss:  0.031153015346203936\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  97\n",
      "Steps taken:  202\n",
      "Cumulative Steps taken:  16769\n",
      "Loss:  5.471672060484335e-06\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  98\n",
      "Steps taken:  124\n",
      "Cumulative Steps taken:  16893\n",
      "Loss:  0.00025655645110428795\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  99\n",
      "Steps taken:  204\n",
      "Cumulative Steps taken:  17097\n",
      "Loss:  9.220992801784639e-06\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  100\n",
      "Steps taken:  162\n",
      "Cumulative Steps taken:  17259\n",
      "Loss:  0.00024100846621118597\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  101\n",
      "Steps taken:  198\n",
      "Cumulative Steps taken:  17457\n",
      "Loss:  0.00016637896701249987\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  102\n",
      "Steps taken:  128\n",
      "Cumulative Steps taken:  17585\n",
      "Loss:  0.030136339723817167\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  103\n",
      "Steps taken:  175\n",
      "Cumulative Steps taken:  17760\n",
      "Loss:  0.00016538435645540531\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  104\n",
      "Steps taken:  228\n",
      "Cumulative Steps taken:  17988\n",
      "Loss:  3.7240174474788244e-05\n",
      "Reward:  3.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  105\n",
      "Steps taken:  175\n",
      "Cumulative Steps taken:  18163\n",
      "Loss:  8.577368534619953e-05\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  106\n",
      "Steps taken:  202\n",
      "Cumulative Steps taken:  18365\n",
      "Loss:  0.0003021659199568906\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  107\n",
      "Steps taken:  174\n",
      "Cumulative Steps taken:  18539\n",
      "Loss:  0.0002512777238419704\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  108\n",
      "Steps taken:  201\n",
      "Cumulative Steps taken:  18740\n",
      "Loss:  0.00018948534648423408\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  109\n",
      "Steps taken:  139\n",
      "Cumulative Steps taken:  18879\n",
      "Loss:  0.00033065968448090497\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  110\n",
      "Steps taken:  150\n",
      "Cumulative Steps taken:  19029\n",
      "Loss:  2.0182601927928173e-05\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  111\n",
      "Steps taken:  204\n",
      "Cumulative Steps taken:  19233\n",
      "Loss:  0.00011237771006504006\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  112\n",
      "Steps taken:  160\n",
      "Cumulative Steps taken:  19393\n",
      "Loss:  0.03166611118144987\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  113\n",
      "Steps taken:  168\n",
      "Cumulative Steps taken:  19561\n",
      "Loss:  0.000262909660584042\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps taken:  174\n",
      "Cumulative Steps taken:  19735\n",
      "Loss:  5.297880276276548e-05\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  115\n",
      "Steps taken:  207\n",
      "Cumulative Steps taken:  19942\n",
      "Loss:  0.00038428119719573065\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  116\n",
      "Steps taken:  191\n",
      "Cumulative Steps taken:  20133\n",
      "Loss:  0.00025861572286822086\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  117\n",
      "Steps taken:  121\n",
      "Cumulative Steps taken:  20254\n",
      "Loss:  7.759468454435046e-05\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  118\n",
      "Steps taken:  230\n",
      "Cumulative Steps taken:  20484\n",
      "Loss:  9.646870660773974e-05\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  119\n",
      "Steps taken:  207\n",
      "Cumulative Steps taken:  20691\n",
      "Loss:  3.809781141312423e-05\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  120\n",
      "Steps taken:  135\n",
      "Cumulative Steps taken:  20826\n",
      "Loss:  0.0003821598704525141\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  121\n",
      "Steps taken:  204\n",
      "Cumulative Steps taken:  21030\n",
      "Loss:  0.0006605796600099565\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  122\n",
      "Steps taken:  194\n",
      "Cumulative Steps taken:  21224\n",
      "Loss:  0.00020968221330786417\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  123\n",
      "Steps taken:  206\n",
      "Cumulative Steps taken:  21430\n",
      "Loss:  0.00012442147061547773\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  124\n",
      "Steps taken:  184\n",
      "Cumulative Steps taken:  21614\n",
      "Loss:  0.00017136491600074815\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  125\n",
      "Steps taken:  199\n",
      "Cumulative Steps taken:  21813\n",
      "Loss:  1.9583979475369794e-05\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  126\n",
      "Steps taken:  192\n",
      "Cumulative Steps taken:  22005\n",
      "Loss:  0.0003208788173351217\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  127\n",
      "Steps taken:  197\n",
      "Cumulative Steps taken:  22202\n",
      "Loss:  0.0001202727722514319\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  128\n",
      "Steps taken:  231\n",
      "Cumulative Steps taken:  22433\n",
      "Loss:  0.00026993287377195466\n",
      "Reward:  3.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  129\n",
      "Steps taken:  173\n",
      "Cumulative Steps taken:  22606\n",
      "Loss:  0.0004910254468466898\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  130\n",
      "Steps taken:  134\n",
      "Cumulative Steps taken:  22740\n",
      "Loss:  3.021231179653502e-05\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  131\n",
      "Steps taken:  145\n",
      "Cumulative Steps taken:  22885\n",
      "Loss:  0.00020107259392756886\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  132\n",
      "Steps taken:  154\n",
      "Cumulative Steps taken:  23039\n",
      "Loss:  2.8882362286329045e-05\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  133\n",
      "Steps taken:  242\n",
      "Cumulative Steps taken:  23281\n",
      "Loss:  0.030984891966942842\n",
      "Reward:  3.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  134\n",
      "Steps taken:  139\n",
      "Cumulative Steps taken:  23420\n",
      "Loss:  0.00034974541886667886\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  135\n",
      "Steps taken:  149\n",
      "Cumulative Steps taken:  23569\n",
      "Loss:  7.376991351559408e-05\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  136\n",
      "Steps taken:  224\n",
      "Cumulative Steps taken:  23793\n",
      "Loss:  0.03183591387754433\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  137\n",
      "Steps taken:  127\n",
      "Cumulative Steps taken:  23920\n",
      "Loss:  6.15554148217199e-06\n",
      "Reward:  0.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  138\n",
      "Steps taken:  169\n",
      "Cumulative Steps taken:  24089\n",
      "Loss:  0.0002485352722734277\n",
      "Reward:  1.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  139\n",
      "Steps taken:  211\n",
      "Cumulative Steps taken:  24300\n",
      "Loss:  0.031240819136403225\n",
      "Reward:  2.0\n",
      "\n",
      " ----------------------------------------\n",
      "Episode  140\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e70cae4bfb98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/hdd/BITS/NNFL/Design Project/Double-DQN/agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;31m#Train primary network every k steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_update\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# # DEBUG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/hdd/BITS/NNFL/Design Project/Double-DQN/agent.py\u001b[0m in \u001b[0;36mbatch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m#Calulating loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_t_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_Q_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# # DEBUG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/hdd/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/hdd/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/hdd/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2203\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2204\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = metrics['rewards']\n",
    "losses = metrics['losses']\n",
    "steps = metrics['steps']\n",
    "cum_steps = metrics['cum_steps']\n",
    "\n",
    "assert len(rewards) == len(losses) == len(steps)\n",
    "\n",
    "episodes_info = list(zip(rewards, losses, steps, cum_steps))\n",
    "for i, (reward, loss, step, cum_step) in enumerate(episodes_info):\n",
    "    print('Episode %d :' % i)\n",
    "    print('Step - ', step)\n",
    "    print('Cumulative steps - ', cum_step)\n",
    "    print('Reward - ', reward)\n",
    "    print('Loss - ', loss)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(losses)), losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(rewards)), rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "metrics_dump_name = 'metrics_dump_breakoutv4_1000_2.data'\n",
    "with open(metrics_dump_name, 'wb') as f:\n",
    "    pickle.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
